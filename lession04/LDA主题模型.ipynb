{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "\n",
    "参考 https://blog.csdn.net/m0_37788308/article/details/78935021\n",
    "\n",
    "https://www.cnblogs.com/gasongjian/p/7631978.html\n",
    "\n",
    "理解LDA，可以分为下述5个步骤： \n",
    "* 一个函数：gamma函数 \n",
    "* 四个分布：二项分布、多项分布、beta分布、Dirichlet分布 \n",
    "* 一个概念和一个理念：共轭先验和贝叶斯框架 \n",
    "* 两个模型：pLSA、LDA \n",
    "* 一个采样：Gibbs采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 共轭先验分布\n",
    "  共轭，顾名思义，两个及以上的对象，互相牵制、控制。 \n",
    "  那在贝叶斯理论里，在已知似然函数情况下（已经有样本数据了），根据先验概率函数求后验概率，问题是：选取什么样的先验分布，会让后验分布与先验分布具有相同的数学形式呢，从这里提出了共轭分布理论。 \n",
    "（x为样本数据，P(x)就是归一化因子（联想全概率$\n",
    "P(x)=\\sum_{i=1}^{n} P\\left(\\theta_{i}\\right) P\\left(x | \\theta_{i}\\right)\n",
    "$，如果不关心P(θ|x)的具体值，只考察θ取何值时后验概率P(θ|x)最大，则可将分母省去。）\n",
    "\n",
    "$$\n",
    "P(\\theta | x)=\\frac{P(x | \\theta) P(\\theta)}{P(x)} \\propto P(x | \\theta) P(\\theta)\n",
    "$$\n",
    "\n",
    "  在贝叶斯概率理论中，如果后验概率P(θ|x)和先验概率p(θ)满足同样的分布律（同分布），那么，先验分布叫作似然函数的共轭先验分布，先验分布和后验分布被叫作共轭分布。\n",
    "\n",
    "  共轭先验的好处主要在于代数上的方便性，可以直接给出后验分布的封闭形式，否则的话只能数值计算。共轭先验也有助于获得关于似然函数如何更新先验分布的直观印象\n",
    "\n",
    "### 先验概率p(θ)\n",
    "\n",
    "  先验概率（prior probability）通俗来讲是指根据以往经验和分析得到的概率分布。就比如询问某高校的男女比例，一个同学回答“3：2”，这个概率很可能就是该同学根据身边同学的性别比例,得到的一个经验概率。\n",
    "\n",
    "### 似然函数\n",
    "\n",
    "  统计学中，似然函数是一种关于统计模型参数的函数，表示模型参数中的似然性。 \n",
    "计算上：给定输出x时，关于参数θ的似然函数L(θ|x)（在数值上）等于给定参数θ后数据X的概率：$L(θ|x)=P(X=x|θ)$。比如拿一枚不确定正反概率的硬币，三正两反的似然函数就是：$C_{5}^{3} p^{3}(1-p)^{2}$（假设正面概率为P）。\n",
    "\n",
    "  简单意思就是，把参数设出来，记为θ，那似然函数就是在参数θ下，样本事件所发生的概率表述。 \n",
    "  \n",
    "  但是我们要注意在统计学中，似然和概率又不一样，概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果，而似然性则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。 \n",
    "  \n",
    "  例如，对于“一枚正反对称的硬币上抛十次”这种事件，我们可以问硬币落地时十次都是正面向上的“概率”是多少；而对于“一枚硬币上抛十次”，我们则可以问，这枚硬币正反面对称的“似然”程度是多少。\n",
    "\n",
    "### 后验概率P(θ|x)\n",
    "  在贝叶斯统计中，一个随机事件或者一个不确定事件的后验概率是在给出相关证据或数据后所得到的条件概率。在使用贝叶斯定理时，我们通过将先验概率与似然函数相乘并归一化，来得到后验概率分布，也就是给出某数据，该不确定量的条件分布。来个例子，计算一下吧:\n",
    "\n",
    "  假设一个学校里有60%男生和40%女生。女生穿裤子的人数和穿裙子的人数相等，所有男生穿裤子。一个人在远处随机看到了一个穿裤子的学生。那么这个学生是女生的概率是多少？ \n",
    "  \n",
    "  使用贝叶斯定理，事件A是看到女生，事件B是看到一个穿裤子的学生。我们所要计算的是P(A|B)，分析一下：\n",
    "  \n",
    "  P(A)是忽略其它因素，看到女生的先验概率，在这里是0.4； \n",
    "  \n",
    "  P(A’)是忽略其它因素，看到不是女生（即看到男生）的概率，在这里是0.6； \n",
    "  \n",
    "  P(B|A)是女生穿裤子的概率，在这里是0.5； \n",
    "  \n",
    "  P(B|A’)是男生穿裤子的概率，在这里是1；\n",
    "  \n",
    "  P(B)是忽略其它因素，学生穿裤子的概率，  $P(B) = P(B|A)P(A) + P(B|A’)P(A’)$（利用全概率公式），在这里是0.5×0.4 + 1×0.6 = 0.8。 \n",
    "  \n",
    "  根据贝叶斯定理，我们计算出后验概率P(A|B)：\n",
    "  $\n",
    "P(A | B)=\\frac{P(A B)}{P(B)}=\\frac{P(B | A) P(A)}{P(B)}\n",
    "$\n",
    "  从这里也可以看出来后验概率其实也是条件概率。\n",
    "  \n",
    "## 先验概率与后验概率的区别\n",
    "\n",
    "  先验概率不是根据有关自然状态的全部资料测定的，而只是利用现有的材料(主要是历史资料)计算的；后验概率使用了有关自然状态更加全面的资料，既有先验概率资料，也有补充资料； \n",
    "  \n",
    "  先验概率的计算比较简单，没有使用贝叶斯公式；而后验概率的计算，要使用贝叶斯公式，而且在利用样本资料计算逻辑概率时，还要使用理论概率分布，需要更多的数理统计知识。\n",
    "  \n",
    "## 同样的分布律如何理解？\n",
    "\n",
    "  后验概率P(θ|x)和先验概率p(θ)满足 同样的分布律（同分布），那么，先验分布和后验分布被叫做共轭分布，在这里的同分布，是指有相同的概率分布形式，比如两枚不均匀硬币，参数P数值不一样，但都服从二项分布，两个分布同分布。\n",
    "  \n",
    "## Beta-Binomial共轭\n",
    "\n",
    "在之前的内容中已经解决了：Beta分布的怎么来？Beta分布和Gamma函数的关系，以及Beta分布的期望，那谁的分布是Beta分布呢？ \n",
    "\n",
    "从Bata分布怎么来中，我们了解了二项分布和Beta分布的关系，它们还有那些更紧密的关系呢？ \n",
    "\n",
    "\n",
    "二项分布/伯努利分布的共轭先验分布是Beta分布。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "\n",
    "来自 https://blog.csdn.net/u013710265/article/details/73480332\n",
    "\n",
    "LDA（Latent Dirichlet Allocation）是一种文档主题生成模型，也称为一个三层贝叶斯概率模型，包含词、主题和文档三层结构。所谓生成模型，就是说，我们认为一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”这样一个过程得到。文档到主题服从多项式分布，主题到词服从多项式分布。\n",
    "\n",
    "LDA是一种非监督机器学习技术，可以用来识别大规模文档集（document collection）或语料库（corpus）中潜藏的主题信息。它采用了词袋（bag of words）的方法，这种方法将每一篇文档视为一个词频向量，从而将文本信息转化为了易于建模的数字信息。但是词袋方法 <u>没有考虑词与词之间的顺序</u> ，这简化了问题的复杂性，同时也为模型的改进提供了契机。每一篇文档代表了一些主题所构成的一个概率分布，而每一个主题又代表了很多单词所构成的一个概率分布。\n",
    "\n",
    "\n",
    "- 1、LDA生成过程\n",
    "\n",
    "对于语料库中的每篇文档，LDA定义了如下生成过程（generativeprocess）：  \n",
    "- (1)对每一篇文档，从主题分布中抽取一个主题;\n",
    "- (2)从上述被抽到的主题所对应的单词分布中抽取一个单词;\n",
    "- (3)重复上述过程直至遍历文档中的每一个单词。  \n",
    "\n",
    "语料库中的每一篇文档与T（通过反复试验等方法事先给定）个主题的一个多项分布 （multinomialdistribution）相对应，将该多项分布记为θ。每个主题又与词汇表（vocabulary）中的V个单词的一个多项分布相对应，将这个多项分布记为φ。\n",
    "\n",
    "- 2、LDA整体流程\n",
    "\n",
    "文档集合D，主题集合T\n",
    "D中每个文档d看作一个单词序列<w1, w2, …… ,wn>，wi表示第i个单词，设d有n个单词。（LDA里面称之为wordbag，实际上每个单词的出现位置对LDA算法无影响）\n",
    "\n",
    "文档集合D中的所有单词组成一个大集合VOCABULARY(简称VOC)。\n",
    "\n",
    "LDA以文档集合D作为输入，希望训练出两个结果向量(设聚成k个topic，VOC中共包含m个词)。\n",
    "\n",
    "$对每个D中的文档d，对应到不同Topic的概率θd<pt1,...,ptk>，其中，pti表示d对应T中第i个topic的概率$。计算方法是直观的，$pti=nti/n，其中nti表示d中对应第i个topic的词的数目，n是d中所有词的总数。$\n",
    "\n",
    "$对每个T中的topic，生成不同单词的概率φt<pw1,...,pwm>，其中，pwi表示t生成VOC中第i个单词的概率。$计算方法同样很直观，$pwi=Nwi/N，其中Nwi表示对应到topict的VOC中第i个单词的数目，N表示所有对应到topict的单词总数。$\n",
    "\n",
    "LDA的核心公式如下：  \n",
    "\n",
    "$p(w|d)=p(w|t)*p(t|d)  $\n",
    "\n",
    "$直观的看这个公式，就是以Topic作为中间层，可以通过当前的θd和φt给出了文档d中出现单词w的概率。其中p(t|d)利用θd计算得到，p(w|t)利用φt计算得到。 $\n",
    "\n",
    "实际上，利用当前的θd和φt，我们可以为一个文档中的一个单词计算它对应任意一个Topic时的p(w|d)，然后根据这些结果来更新这个词应该对应的topic。然后，如果这个更新改变了这个单词所对应的Topic，就会反过来影响θd和φt。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8e15c9d32566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\\\\source_wzp_lda_1.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lda'"
     ]
    }
   ],
   "source": [
    "# 1.获取训练矩阵和单词\n",
    "\n",
    "import numpy as np\n",
    "import lda\n",
    " \n",
    "X = np.genfromtxt(\"data\\\\source_wzp_lda_1.txt\", skip_header=1, dtype = np.int)\n",
    " \n",
    "#the vocab\n",
    "file_vocab = open(\"data\\\\vectoritems_lda_3.txt\", \"r\")\n",
    "vocab = (file_vocab.read().decode(\"utf-8\").split(\"\\n\"))[0:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X是一个n*m的矩阵，表示有n个文本，m个单词，值表示出现次数或者是否出现。\n",
    "vocab是m个单词组成的list。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2、训练数据，指定主题，进行迭代\n",
    "#指定11个主题，500次迭代\n",
    "model = lda.LDA(random_state=1, n_topics=11, n_iter=1000)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 主题-单词（topic-word）分布\n",
    "topic_word = model.topic_word_\n",
    "print(\"type(topic_word): {}\".format(type(topic_word)))\n",
    "print(\"shape: {}\".format(topic_word.shape))\n",
    " \n",
    "#获取每个topic下权重最高的10个单词\n",
    "n = 10\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]\n",
    "    print(\"topic {}\\n- {}\".format(i, ('-'.join(topic_words)).encode(\"utf-8\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.文档主题(Document-Topic)分布：\n",
    "doc_topic = model.doc_topic_\n",
    "print(\"type(doc_topic): {}\".format(type(doc_topic)))\n",
    "print(\"shape: {}\".format(doc_topic.shape))\n",
    " \n",
    "#一篇文章对应一行，每行的和为1\n",
    "#输入前10篇文章最可能的Topic\n",
    "for n in range(20):\n",
    "    '''\n",
    "    for i in doc_topic[n]:\n",
    "        print i\n",
    "        '''\n",
    "    topic_most_pr = doc_topic[n].argmax()\n",
    "    print(\"doc: {} topic: {}\".format(n, topic_most_pr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
