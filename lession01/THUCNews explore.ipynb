{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN中文文本分类（Tensorflow）\n",
    "\n",
    "THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档。本文采用了清华NLP组提供的THUCNews新闻文本分类数据集的一个子集."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.keras as kr\n",
    "\n",
    "if sys.version_info[0] > 2:\n",
    "    is_py3 = True\n",
    "else:\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding(\"utf-8\")\n",
    "    is_py3 = False\n",
    "\n",
    "\n",
    "def native_word(word, encoding='utf-8'):\n",
    "    \"\"\"如果在python2下面使用python3训练的模型，可考虑调用此函数转化一下字符编码\"\"\"\n",
    "    if not is_py3:\n",
    "        return word.encode(encoding)\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "def native_content(content):\n",
    "    if not is_py3:\n",
    "        return content.decode('utf-8')\n",
    "    else:\n",
    "        return content\n",
    "\n",
    "\n",
    "def open_file(filename, mode='r'):\n",
    "    \"\"\"\n",
    "    常用文件操作，可在python2和python3间切换.\n",
    "    mode: 'r' or 'w' for read or write\n",
    "    \"\"\"\n",
    "    if is_py3:\n",
    "        return open(filename, mode, encoding='utf-8', errors='ignore')\n",
    "    else:\n",
    "        return open(filename, mode)\n",
    "\n",
    "\n",
    "def read_file(filename):\n",
    "    \"\"\"读取文件数据\"\"\"\n",
    "    contents, labels = [], []\n",
    "    with open_file(filename) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                label, content = line.strip().split('\\t')\n",
    "                if content:\n",
    "                    contents.append(list(native_content(content)))\n",
    "                    labels.append(native_content(label))\n",
    "            except:\n",
    "                pass\n",
    "    return contents, labels\n",
    "\n",
    "\n",
    "def build_vocab(train_dir, vocab_dir, vocab_size=5000):\n",
    "    \"\"\"根据训练集构建词汇表，存储\"\"\"\n",
    "    data_train, _ = read_file(train_dir)\n",
    "\n",
    "    all_data = []\n",
    "    for content in data_train:\n",
    "        all_data.extend(content)\n",
    "\n",
    "    counter = Counter(all_data)\n",
    "    count_pairs = counter.most_common(vocab_size - 1)\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    # 添加一个 <PAD> 来将所有文本pad为同一长度\n",
    "    words = ['<PAD>'] + list(words)\n",
    "    open_file(vocab_dir, mode='w').write('\\n'.join(words) + '\\n')\n",
    "\n",
    "\n",
    "def read_vocab(vocab_dir):\n",
    "    \"\"\"读取词汇表\"\"\"\n",
    "    # words = open_file(vocab_dir).read().strip().split('\\n')\n",
    "    with open_file(vocab_dir) as fp:\n",
    "        # 如果是py2 则每个值都转化为unicode\n",
    "        words = [native_content(_.strip()) for _ in fp.readlines()]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id\n",
    "\n",
    "\n",
    "def read_category():\n",
    "    \"\"\"读取分类目录，固定\"\"\"\n",
    "    categories = ['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
    "\n",
    "    categories = [native_content(x) for x in categories]\n",
    "\n",
    "    cat_to_id = dict(zip(categories, range(len(categories))))\n",
    "\n",
    "    return categories, cat_to_id\n",
    "\n",
    "\n",
    "def to_words(content, words):\n",
    "    \"\"\"将id表示的内容转换为文字\"\"\"\n",
    "    return ''.join(words[x] for x in content)\n",
    "\n",
    "\n",
    "def process_file(filename, word_to_id, cat_to_id, max_length=600):\n",
    "    \"\"\"将文件转换为id表示，转换为字向量\"\"\"\n",
    "    # 读取整个文件\n",
    "    contents, labels = read_file(filename)\n",
    "\n",
    "    data_id, label_id = [], []\n",
    "    for i in range(len(contents)):\n",
    "        # 处理每个新闻\n",
    "        # 顺序记录 每个新闻的所包含的字序列表；以及类别编号\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n",
    "        label_id.append(cat_to_id[labels[i]])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度；格式化数据\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, max_length)\n",
    "    y_pad = kr.utils.to_categorical(label_id, num_classes=len(cat_to_id))  # 将标签转换为one-hot表示\n",
    "\n",
    "    return x_pad, y_pad\n",
    "\n",
    "\n",
    "def batch_iter(x, y, batch_size=64):\n",
    "    \"\"\"生成批次数据\"\"\"\n",
    "    data_len = len(x)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_shuffle = x[indices]\n",
    "    y_shuffle = y[indices]\n",
    "\n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cnews/cnews.train.txt: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!head cnews/cnews.train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集\n",
    "train_data = pd.read_csv('../cnews/cnews.train.txt', sep=\"\t\", names=['category', 'content'])\n",
    "# 测试集\n",
    "test_data = pd.read_csv('../cnews/cnews.test.txt', sep=\"\t\", names=['category', 'content'])\n",
    "# 验证集\n",
    "val_data = pd.read_csv('../cnews/cnews.val.txt', sep=\"\t\", names=['category', 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['体育', '娱乐', '家居', '房产', '教育', '时尚', '时政', '游戏', '科技', '财经'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['体育', '娱乐', '家居', '房产', '教育', '时尚', '时政', '游戏', '科技', '财经'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['体育', '娱乐', '家居', '房产', '教育', '时尚', '时政', '游戏', '科技', '财经'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 类别\n",
    "\n",
    "train_category = train_data['category'].unique()\n",
    "test_category = test_data['category'].unique()\n",
    "val_category = val_data['category'].unique()\n",
    "display(train_category, test_category, val_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_shape: (50000, 2)\n",
      "test_data_shape: (10000, 2)\n",
      "val_data_shape: (5000, 2)\n"
     ]
    }
   ],
   "source": [
    "# 数据集大小\n",
    "print('train_data_shape:', train_data.shape)\n",
    "print('test_data_shape:', test_data.shape)\n",
    "print('val_data_shape:', val_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐'],\n",
       " {'体育': 0,\n",
       "  '财经': 1,\n",
       "  '房产': 2,\n",
       "  '家居': 3,\n",
       "  '教育': 4,\n",
       "  '科技': 5,\n",
       "  '时尚': 6,\n",
       "  '时政': 7,\n",
       "  '游戏': 8,\n",
       "  '娱乐': 9})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新闻类别映射数字，\n",
    "\n",
    "categories, cat_to_id = read_category()\n",
    "categories, cat_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f6de468ec021>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 读取词表，并赋予一个id编号\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../cnews/cnews.vocab.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'read_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "# 读取词表，并赋予一个id编号\n",
    "words, word_to_id = read_vocab(vocab_dir='../cnews/cnews.vocab.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集\n",
    "\n",
    "config.vocab_size = len(words)\n",
    "x_train, y_train = process_file('../cnews/cnews.train.txt', word_to_id, cat_to_id, config.vocab_size)\n",
    "x_val, y_val = process_file('../cnews/cnews.val.txt', word_to_id, cat_to_id,config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf 执行\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "\n",
    "# 创建session\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "total_batch = 0  # 总批次\n",
    "best_acc_val = 0.0  # 最佳验证集准确率\n",
    "last_improved = 0  # 记录上一次提升批次\n",
    "require_improvement = 1000  # 如果超过1000轮未提升，提前结束训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
